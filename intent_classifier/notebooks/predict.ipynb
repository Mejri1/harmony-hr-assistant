{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de28acdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Omar\\Desktop\\ai-attrition-system\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Omar\\Desktop\\ai-attrition-system\\venv\\Lib\\site-packages\\transformers\\__init__.py:1022\u001b[39m\n\u001b[32m   1018\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m   1020\u001b[39m _import_structure = {k: \u001b[38;5;28mset\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _import_structure.items()}\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m import_structure = \u001b[43mdefine_import_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[34;43m__file__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1023\u001b[39m import_structure[\u001b[38;5;28mfrozenset\u001b[39m({})].update(_import_structure)\n\u001b[32m   1025\u001b[39m sys.modules[\u001b[34m__name__\u001b[39m] = _LazyModule(\n\u001b[32m   1026\u001b[39m     \u001b[34m__name__\u001b[39m,\n\u001b[32m   1027\u001b[39m     \u001b[38;5;28mglobals\u001b[39m()[\u001b[33m\"\u001b[39m\u001b[33m__file__\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1030\u001b[39m     extra_objects={\u001b[33m\"\u001b[39m\u001b[33m__version__\u001b[39m\u001b[33m\"\u001b[39m: __version__},\n\u001b[32m   1031\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Omar\\Desktop\\ai-attrition-system\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2703\u001b[39m, in \u001b[36mdefine_import_structure\u001b[39m\u001b[34m(module_path, prefix)\u001b[39m\n\u001b[32m   2679\u001b[39m \u001b[38;5;129m@lru_cache\u001b[39m\n\u001b[32m   2680\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefine_import_structure\u001b[39m(module_path: \u001b[38;5;28mstr\u001b[39m, prefix: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m) -> IMPORT_STRUCTURE_T:\n\u001b[32m   2681\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2682\u001b[39m \u001b[33;03m    This method takes a module_path as input and creates an import structure digestible by a _LazyModule.\u001b[39;00m\n\u001b[32m   2683\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2701\u001b[39m \u001b[33;03m    If `prefix` is not None, it will add that prefix to all keys in the returned dict.\u001b[39;00m\n\u001b[32m   2702\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2703\u001b[39m     import_structure = \u001b[43mcreate_import_structure_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2704\u001b[39m     spread_dict = spread_import_structure(import_structure)\n\u001b[32m   2706\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Omar\\Desktop\\ai-attrition-system\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2416\u001b[39m, in \u001b[36mcreate_import_structure_from_path\u001b[39m\u001b[34m(module_path)\u001b[39m\n\u001b[32m   2414\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os.listdir(module_path):\n\u001b[32m   2415\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m f != \u001b[33m\"\u001b[39m\u001b[33m__pycache__\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m os.path.isdir(os.path.join(module_path, f)):\n\u001b[32m-> \u001b[39m\u001b[32m2416\u001b[39m         import_structure[f] = \u001b[43mcreate_import_structure_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2418\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isdir(os.path.join(directory, f)):\n\u001b[32m   2419\u001b[39m         adjacent_modules.append(f)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Omar\\Desktop\\ai-attrition-system\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2440\u001b[39m, in \u001b[36mcreate_import_structure_from_path\u001b[39m\u001b[34m(module_path)\u001b[39m\n\u001b[32m   2437\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m module_name.endswith(\u001b[33m\"\u001b[39m\u001b[33m.py\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   2438\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2440\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m   2441\u001b[39m     file_content = f.read()\n\u001b[32m   2443\u001b[39m \u001b[38;5;66;03m# Remove the .py suffix\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:309\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, errors)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Load the latest checkpoint\n",
    "MODEL_DIR = \"../models/intent_classifier/final\"  # update if needed\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Label map (adjust if you used different label order)\n",
    "id2label = {\n",
    "    0: \"needs_rag\",\n",
    "    1: \"no_rag\"\n",
    "}\n",
    "\n",
    "# Function to predict intent\n",
    "def predict_intent(text: str):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "        pred_id = torch.argmax(probs, dim=1).item()\n",
    "        confidence = probs[0][pred_id].item()\n",
    "    return id2label[pred_id], round(confidence, 3)\n",
    "\n",
    "# Test inputs\n",
    "examples = [\n",
    "    \"Hey! Just checking in, how are things on your side?\",\n",
    "    \"Lovely weather today, isn‚Äôt it?\",\n",
    "    \"Where can I find the updated health insurance policy for 2025?\",\n",
    "    \"I need information about how salary reviews are done here.\",\n",
    "    \"I‚Äôm feeling extremely drained after every workday lately.\",\n",
    "    \"Honestly, I don‚Äôt think I can keep up this pace anymore.\",\n",
    "    \"You guys are useless, nothing ever works here.\",\n",
    "    \"This is a dumb bot, waste of time.\"\n",
    "]\n",
    "\n",
    "# Run predictions\n",
    "for text in examples:\n",
    "    label, confidence = predict_intent(text)\n",
    "    print(f\"üó£Ô∏è \\\"{text}\\\"\\n‚Üí Predicted: {label} (Confidence: {confidence})\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae0837c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Predicted intent: moderation_required (Confidence: 0.996)\n",
      "\n",
      "üß† Predicted intent: moderation_required (Confidence: 0.916)\n",
      "\n",
      "üß† Predicted intent: no_rag (Confidence: 0.863)\n",
      "\n",
      "üß† Predicted intent: moderation_required (Confidence: 0.758)\n",
      "\n",
      "üß† Predicted intent: needs_rag (Confidence: 0.903)\n",
      "\n",
      "üß† Predicted intent: needs_rag (Confidence: 0.876)\n",
      "\n",
      "üß† Predicted intent: no_rag (Confidence: 0.981)\n",
      "\n",
      "üß† Predicted intent: needs_rag (Confidence: 0.955)\n",
      "\n",
      "üß† Predicted intent: no_rag (Confidence: 0.522)\n",
      "\n",
      "‚ö†Ô∏è Please enter a non-empty message.\n"
     ]
    }
   ],
   "source": [
    "# Run this in a Jupyter notebook cell\n",
    "\n",
    "while True:\n",
    "    text = input(\"üìù Enter a message (or type 'exit' to quit): \").strip()\n",
    "    if text.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"üëã Exiting...\")\n",
    "        break\n",
    "\n",
    "    if not text:\n",
    "        print(\"‚ö†Ô∏è Please enter a non-empty message.\")\n",
    "        continue\n",
    "\n",
    "    label, confidence = predict_intent(text)\n",
    "    print(f\"üß† Predicted intent: {label} (Confidence: {confidence})\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014dd149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "c:\\Users\\Omar\\Desktop\\ai-attrition-system\\venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "moderation_required       0.99      1.00      1.00       120\n",
      "          needs_rag       1.00      1.00      1.00       104\n",
      "             no_rag       1.00      0.99      0.99        97\n",
      "\n",
      "           accuracy                           1.00       321\n",
      "          macro avg       1.00      1.00      1.00       321\n",
      "       weighted avg       1.00      1.00      1.00       321\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import TextClassificationPipeline\n",
    "from datasets import Dataset\n",
    "\n",
    "# üìÅ Paths\n",
    "TEST_PATH = \"../data/test_split.json\"\n",
    "\n",
    "# üîπ Load test data\n",
    "with open(TEST_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "test_df = pd.DataFrame(test_data)\n",
    "texts = test_df[\"text\"].tolist()\n",
    "true_labels = test_df[\"label\"].tolist()\n",
    "\n",
    "# üîÑ Map string labels to numeric\n",
    "label_names = model.config.id2label\n",
    "label2id = {v: int(k) for k, v in label_names.items()}\n",
    "\n",
    "# üß† Predict using pipeline\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=False, device=0 if torch.cuda.is_available() else -1)\n",
    "preds = pipe(texts)\n",
    "\n",
    "# üéØ Extract predicted labels\n",
    "predicted_labels = [pred[\"label\"] for pred in preds]\n",
    "\n",
    "# üìä Show evaluation metrics\n",
    "print(classification_report(true_labels, predicted_labels, target_names=list(label2id.keys())))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
